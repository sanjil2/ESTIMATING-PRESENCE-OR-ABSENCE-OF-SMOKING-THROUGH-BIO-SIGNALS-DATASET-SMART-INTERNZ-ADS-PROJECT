# -*- coding: utf-8 -*-
"""smoke_detection.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3RHLGUX5kb7TGUXh01qwYUM_aM84srY

![picture.jpg](https://drive.google.com/uc?=export=view&id=1Vcy1H5KLsT9p-xQoLemsluY4X3bChQuw)

<h1><b> VIT CHENNAI</b> </h1>
<h1>
  <b>Estimating  Presence or Absence of Smoking Through Bio Signal Dataset</b>
</h1>

<h5>
SmartInternz Applied Data Science Project
</h5>
<br>

<h6><b>SANJIL KC</b> (20BCE1855)</h6>
<h6><b>ABRAR AHAMED</b>   (20BCE1437) </h6>
<h6><b>SALUGU JANAKI MANOJ</b> (20BEC1213) </h6>
<h6><b>HRITHIK UMESH</b> (20BCE1319) </h6>

<br>
<br>

# Introduction
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  #for data manipulation operations
import numpy as np  #for numeric operations on data
from collections import Counter #for outliers detection
import seaborn as sns  #for data visualization operations
import matplotlib.pyplot as plt  #for data visualization operations
import plotly.express as px #for scatter 3d
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder # for encoding and standardization
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from prettytable import PrettyTable
# %matplotlib inline

# to ignore warnings
import warnings
warnings.filterwarnings("ignore")

"""## Reading the dataset"""

df = pd.read_csv('smoking.csv')
df.head(n = 10).style.background_gradient(cmap = "Blues")

"""# Analyze the dataset and drop unecessary column"""

# Drop ID
df = df.drop("ID", axis = 1)

df.columns

"""####
#### We see thet the dataset attributes are divided into dependent and independent attributes.
#### The inedependent attributes are :
- `ID` : index
- `Gender`: The gender of the individual.
- `Age`: Age of the individual categorized into 5-year gaps.
- `Height`: Height of the individual in centimeters.
- `Weight`: Weight of the individual in kilograms.
- `Waist circumference`: Measurement of the individual's waist circumference in centimeters.
- `Eyesight (left)`: Assessment of the individual's eyesight in the left eye.
- `Eyesight (right)`: Assessment of the individual's eyesight in the right eye.
- `Hearing (left)`: Assessment of the individual's hearing ability in the left ear.
- `Hearing (right)`: Assessment of the individual's hearing ability in the right ear.
- `Systolic Blood Pressure`: Measurement of the pressure in the arteries when the heart beats.
- `Diastolic Blood Pressure`: Measurement of the pressure in the arteries when the heart is at rest.
- `Fasting Blood Sugar`: Measurement of blood sugar level after an overnight fast.
- `Cholesterol (total)`: Total cholesterol level in the blood.
- `Triglyceride`: Measurement of the level of triglycerides in the blood.
- `HDL Cholesterol`: Measurement of high-density lipoprotein (HDL) cholesterol level.
- `LDL Cholesterol`: Measurement of low-density lipoprotein (LDL) cholesterol level.
- `Hemoglobin`: Measurement of the amount of hemoglobin in the blood.
- `Urine Protein`: Presence of protein in the urine.
- `Serum Creatinine`: Measurement of the creatinine level in the blood.
- `AST` (glutamic oxaloacetic transaminase): Measurement of liver enzyme AST level.
- `ALT` (glutamic oxaloacetic transaminase): Measurement of liver enzyme ALT level.
- `GTP`: an energy-rich nucleotide analogous to ATP.
- `Oral Examination Status`: Status of oral examination.
- `Dental Caries`: Presence or absence of dental caries (cavities).
- `Tartar Status`: Status of tartar (hardened plaque) accumulation on teeth.<br>
#### The dependent attribute or (The target variable) is :
- `smoking`: NO smoking = N, YES smoking = Y

# Data Cleaning
"""

# Checking the nulls
df.isnull().sum()

df.shape

# Droping the redundant data
df = df.drop_duplicates()

df.shape

"""##### We see that there is no nulls in our dataset & we have removed the redundant data from our dataset<br>

### Checking and Removing the Outliers
"""

df.head(n = 10).style.background_gradient(cmap = "Blues")

# Specify the columns to plot
numeric_cols = ['age', 'height(cm)', 'weight(kg)', 'waist(cm)', 'eyesight(left)', 'eyesight(right)', 'hearing(left)', 'hearing(right)', 'systolic', 'relaxation', 'fasting blood sugar', 'Cholesterol', 'triglyceride', 'HDL', 'LDL', 'hemoglobin', 'Urine protein', 'AST', 'ALT', 'Gtp', 'dental caries']

# Create a figure and axis object
fig, ax = plt.subplots(figsize=(28,8))

# Create the boxplot
df[numeric_cols].boxplot(ax=ax)

# Set the plot title
ax.set_title('Boxplot of Numerical Columns')

# Show the plot
plt.show()

"""#### Plotting each column"""

# Loop over each column in the DataFrame
for column_name in numeric_cols:

    sns.set_style("whitegrid")
    sns.kdeplot(data=df[column_name])
    plt.xlabel("Values")
    plt.title("Density plot of " + column_name)
    plt.show()

"""##### Here we use IQR to detect the outliers and be able to remove them"""

def detect_outliers(df, min_outlier_occurrences, columns):
    outlier_rows = []
    for col in columns:
        q1 = np.nanpercentile(df[col], 25)
        q3 = np.nanpercentile(df[col], 75)
        iqr = q3 - q1
        outlier_point = 1.5 * iqr
        outliers = df[(df[col] < q1 - outlier_point) | (df[col] > q3 + outlier_point)].index
        outlier_rows.extend(outliers)

    outlier_counts = Counter(outlier_rows)
    outlier_rows_to_drop = [row for row, count in outlier_counts.items() if count >= min_outlier_occurrences]

    return outlier_rows_to_drop


outlier_rows = detect_outliers(df, 5, df.select_dtypes(["float", "int"]).columns)
df = df.drop(outlier_rows)

df.isnull().sum()

df.shape

"""- As we can see, we removed the outliers ..

# Data Exploration and Visualization
"""

df.describe()

df.info()

df.groupby('gender').size().plot(kind='pie', explode=[0,0.1], autopct='%1.1f%%', shadow=True, colors=["pink", "gray"], title="Gender")

df.groupby('tartar').size().plot(kind='bar', title="Tartar", rot=0, xlabel='Age', ylabel='Count')

fig, axes = plt.subplots(2, 3, figsize = (20, 12))
axes = axes.flatten()

sns.scatterplot(ax = axes[0], x = "relaxation", y = "hemoglobin",
                hue = "smoking", size = "gender", sizes=(20, 100), legend="full",
                data = df).set(title = "Relationship between 'Relaxation' and 'hemoglobin'");

sns.scatterplot(ax = axes[1], x = "systolic", y = "Cholesterol", hue = "smoking",
                size = "gender", sizes=(20, 100), legend="full",
                data = df).set(title = "Relationship between 'Systolic' and 'cholesterol'");

sns.scatterplot(ax = axes[2], x = "LDL", y = "Urine protein", hue = "smoking",
                size = "gender", sizes=(20, 100), legend="full",
                data = df).set(title = "Relationship between 'LDL' and 'urine protein'");

sns.scatterplot(ax = axes[3], x = "HDL", y = "serum creatinine", hue = "smoking",
                size = "gender", sizes=(20, 100), legend="full",
                data = df).set(title = "Relationship between 'HDL' and 'serum creatinine'");

sns.scatterplot(ax = axes[4], x = "weight(kg)", y = "Gtp", hue = "smoking",
                size = "gender", sizes=(20, 100), legend="full",
                data = df).set(title = "Relationship between 'GTP' and 'age'");

sns.scatterplot(ax = axes[5], x = "AST", y = "fasting blood sugar", hue = "smoking",
                size = "gender", sizes=(20, 100), legend="full",
                data = df).set(title = "Relationship between 'AST' and 'fasting blood sugar'");

df.hist(figsize = (20, 20), bins = 12, legend = False);

fig, axes = plt.subplots(2, 2, figsize = (12, 12))
axes = axes.flatten()

sns.barplot(ax = axes[0],
            x = df["gender"].value_counts().index,
            y = df["gender"].value_counts(),
            data = df, saturation = 1).set(title = "Frequency of classes of the 'gender' variable");

sns.barplot(ax = axes[1],
            x = df["tartar"].value_counts().index,
            y = df["tartar"].value_counts(),
            data = df, saturation = 1).set(title = "Frequency of cases of tartar");

sns.barplot(ax = axes[2],
            x = df["dental caries"].value_counts().index,
            y = df["dental caries"].value_counts(),
            data = df, saturation = 1).set(title = "Frequency of cases of dental caries");

sns.barplot(ax = axes[3],
            x = df["smoking"].value_counts().index,
            y = df["smoking"].value_counts(),
            data = df, saturation = 1).set(title = "Frequency of classes of the 'smoking' variable");

fig = px.scatter_3d(df,
                    x = "hemoglobin",
                    y = "age",
                    z = "Cholesterol",
                    color="smoking")
fig.show();

"""### Observations based on the last explorations:<br>
1. The % of smokers within each blood pressure group is the highest among those who have hypertensive crisis and the least among those with normal blood pressure
2. Maximum people in the dataset have normal blood pressure
3. The highest number of nonsmokers is in the normal blood pressure
    Information source: https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings
4. HDL (the good cholesterol) is higher for non-smokers.
5. Average LDL (the bad cholesterol) is higher for smokers.
6. AST and ALT are healthy for both smoker and non-smoker groups. GTP is at risk for smokers.

# Data Preprocessing

## Encoding and Splitting the data
"""

lbe = LabelEncoder()
lbe.fit_transform(df["gender"])
df["gender"] = lbe.fit_transform(df["gender"])
lbe.fit_transform(df["tartar"])
df["tartar"] = lbe.fit_transform(df["tartar"])
lbe.fit_transform(df["oral"])
df["oral"] = lbe.fit_transform(df["oral"])
lbe.fit_transform(df["smoking"])
df["smoking"] = lbe.fit_transform(df["smoking"])

df.iloc[:3,10:]

# select dependent variable (label)
y = df["smoking"]

# select independent variable (estimator)
x = df.drop("smoking", axis = 1)

# Train-Test-Split
x_train, x_test, y_train, y_test  = train_test_split(x, y, test_size=0.1, shuffle = True, random_state=1)

"""## Scaling the data"""

scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

"""# Training Classification Models

## Support Vector Machine (SVM) Model
"""

svm = SVC(kernel='rbf')
# fit the model with data
svm.fit(x_train, y_train)
# predict with test dataset
y_predict_SVM = svm.predict(x_test)
print(classification_report(y_test,y_predict_SVM))
accuracy_Score_SVM = metrics.accuracy_score(y_test, y_predict_SVM)
print('SVM model accuracy is: {:.2f}%'.format(accuracy_Score_SVM*100))

"""### Confusion Matrix for SVM"""

cm = metrics.confusion_matrix(y_test, y_predict_SVM)
print('Confusion Matrix for SVM :\n', cm, '\n')
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.5)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix of SVM', fontsize=15)
plt.show()

"""## Random Forest Classifier  Model"""

models = RandomForestClassifier(n_estimators=700)
models.fit(x_train, y_train)
# predict with test dataset
y_predict_random = models.predict(x_test)
print(classification_report(y_test,y_predict_random))
accuracy_Score_random = metrics.accuracy_score(y_test, y_predict_random)
print('RandomForest model accuracy is: {:.2f}%'.format(accuracy_Score_random*100))

"""### Confusion Matrix for Random Forest Classifier"""

cm = metrics.confusion_matrix(y_test, y_predict_random)
print('Confusion Matrix for Random Forest Classifier :\n', cm, '\n')
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.5)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix of Random Forest Classifier', fontsize=15)
plt.show()

"""### XGBoost model

#### Why we used this model to this data ?<br>
1. `High-performance`: XGBoost is known for delivering accurate predictions and has a strong track record in machine learning competitions and real-world applications.

2. `Handling complex relationships`: XGBoost is effective at capturing intricate patterns and relationships between different features in the dataset.

3. `Feature importance`: XGBoost can identify the most important factors in making predictions, helping us understand the key signals related to smoking behavior.

4. `Handling imbalanced data`: XGBoost has techniques to handle imbalanced datasets, where the number of samples in different classes is skewed, improving its performance in classifying smoking behavior.

5. `Flexibility and customization`: XGBoost provides a wide range of settings that can be adjusted to optimize model performance for the specific dataset, allowing us to fine-tune the model accordingly.

6. `Ensemble learning`: XGBoost utilizes an ensemble of decision trees, combining their predictions to make a final decision. This ensemble approach helps to reduce overfitting and improve generalization performance.
"""

xgb_model = XGBClassifier(n_estimators = 800)
xgb_model.fit(x_train, y_train)
pred_xgb = xgb_model.predict(x_test)
print(classification_report(y_test,pred_xgb))
accuracy_Score_xgb = metrics.accuracy_score(y_test, pred_xgb)
print('XGBoosting model accuracy is: {:.2f}%'.format(accuracy_Score_xgb*100))

"""Confusion Matrix for XGBoost model"""

cm = metrics.confusion_matrix(y_test, pred_xgb)
print('Confusion Matrix for XGBoosting Classifier :\n', cm, '\n')
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.5)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix of XGBoosting Classifier', fontsize=15)
plt.show()

"""### ROC Curve"""

from sklearn.metrics import roc_curve, roc_auc_score
fpr, tpr, thresholds = roc_curve(y_test, pred_xgb)
auc = roc_auc_score(y_test, pred_xgb)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""### Feature Importance"""

# Get the feature importance scores
importance_scores = xgb_model.feature_importances_

# Get the names of the features
feature_names = x.columns

# Create a dictionary with feature names as keys and importance scores as values
feature_importance = dict(zip(feature_names, importance_scores))

# Sort the features based on their importance scores
sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)

# Print the feature importance in descending order
for feature, importance in sorted_features:
    print(f"{feature}: {importance}")

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(range(len(importance_scores)), importance_scores, align='center')
plt.yticks(range(len(feature_names)), feature_names)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance Scores')
plt.show()

x = PrettyTable()
print('\n')
print("Comparison of Models Results in %")
x.field_names = ["Model", "Accuracy"]

x.add_row(["Random Forest", round(accuracy_score(y_test, models.predict(x_test))*100, 2)])
x.add_row(["SVM", round(accuracy_score(y_test, svm.predict(x_test))*100, 2)])
x.add_row(["XGBoost", round(accuracy_score(y_test, xgb_model.predict(x_test))*100, 2)])

print(x)
print('\n')

"""## **Testing with sample input**"""

data = [1, 40, 155,	60,	81.3,	1.2, 1.0, 1.0, 1.0, 114.0, 73.0,
                 94.0,	215.0, 82.0, 73.0, 126.0, 12.9,	1.0, 0.7, 18.0,	25.0, 27.0, 0, 0, 1	]
input = [int(x) for x in data]
final = [np.array(input)]
print(models.predict(final))

"""### **Saving the Model**"""

import pickle
filename = 'final_model_1.pkl'
pickle.dump(models, open(filename, 'wb'))

